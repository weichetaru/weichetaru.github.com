{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network - Generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python imports\n",
    "import numpy as np # Matrix and vector computation package\n",
    "import matplotlib.pyplot as plt  # Plotting library\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)\n",
    "from sklearn import datasets, cross_validation, metrics # data and evaluation utils\n",
    "from matplotlib.colors import colorConverter, ListedColormap # some plotting functions\n",
    "import itertools\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the data from scikit-learn.\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# Load the targets.\n",
    "# Note that the targets are stored as digits, these need to be \n",
    "#  converted to one-hot-encoding for the output sofmax layer.\n",
    "T = np.zeros((digits.target.shape[0],10))\n",
    "T[np.arange(len(T)), digits.target] += 1\n",
    "\n",
    "# Divide the data into a train and test set.\n",
    "X_train, X_test, T_train, T_test = cross_validation.train_test_split(\n",
    "    digits.data, T, test_size=0.4)\n",
    "# Divide the test set into a validation set and final test set.\n",
    "X_validation, X_test, T_validation, T_test = cross_validation.train_test_split(\n",
    "    X_test, T_test, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAykAAABsCAYAAABuF/0oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRUgBAABK+Q+ghyUWCPS3LQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cc8c4e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot an example of each image.\n",
    "fig = plt.figure(figsize=(10, 1), dpi=100)\n",
    "for i in range(10):\n",
    "    ax = fig.add_subplot(1,10,i+1)\n",
    "    ax.matshow(digits.images[i], cmap='binary') \n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the non-linear functions used\n",
    "def logistic(z): \n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def logistic_deriv(y):  # Derivative of logistic function\n",
    "    return np.multiply(y, (1 - y))\n",
    "    \n",
    "def softmax(z): \n",
    "    return np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the layers used in this model\n",
    "class Layer(object):\n",
    "    \"\"\"Base class for the different layers.\n",
    "    Defines base methods and documentation of methods.\"\"\"\n",
    "    \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters (if any).\n",
    "        The iterator has the same order as get_params_grad.\n",
    "        The elements returned by the iterator are editable in-place.\"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"Return a list of gradients over the parameters.\n",
    "        The list has the same order as the get_params_iter iterator.\n",
    "        X is the input.\n",
    "        output_grad is the gradient at the output of this layer.\n",
    "        \"\"\"\n",
    "        return []\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step linear transformation.\n",
    "        X is the input.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad=None, T=None):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\n",
    "        Y is the pre-computed output of this layer (not needed in this case).\n",
    "        output_grad is the gradient at the output of this layer \n",
    "         (gradient at input of next layer).\n",
    "        Output layer uses targets T to compute the gradient based on the \n",
    "         output error instead of output_grad\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LinearLayer(Layer):\n",
    "    \"\"\"The linear layer performs a linear transformation to its input.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_in, n_out):\n",
    "        \"\"\"Initialize hidden layer parameters.\n",
    "        n_in is the number of input variables.\n",
    "        n_out is the number of output variables.\"\"\"\n",
    "        self.W = np.random.randn(n_in, n_out) * 0.1\n",
    "        self.b = np.zeros(n_out)\n",
    "        \n",
    "    def get_params_iter(self):\n",
    "        \"\"\"Return an iterator over the parameters.\"\"\"\n",
    "        return itertools.chain(np.nditer(self.W, op_flags=['readwrite']),\n",
    "                               np.nditer(self.b, op_flags=['readwrite']))\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step linear transformation.\"\"\"\n",
    "        return X.dot(self.W) + self.b\n",
    "        \n",
    "    def get_params_grad(self, X, output_grad):\n",
    "        \"\"\"Return a list of gradients over the parameters.\"\"\"\n",
    "        JW = X.T.dot(output_grad)\n",
    "        Jb = np.sum(output_grad, axis=0)\n",
    "        return [g for g in itertools.chain(np.nditer(JW), np.nditer(Jb))]\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return output_grad.dot(self.W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticLayer(Layer):\n",
    "    \"\"\"The logistic layer applies the logistic function to its inputs.\"\"\"\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return logistic(X)\n",
    "    \n",
    "    def get_input_grad(self, Y, output_grad):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return np.multiply(logistic_deriv(Y), output_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SoftmaxOutputLayer(Layer):\n",
    "    \"\"\"The softmax output layer computes the classification propabilities at the output.\"\"\"\n",
    "    \n",
    "    def get_output(self, X):\n",
    "        \"\"\"Perform the forward step transformation.\"\"\"\n",
    "        return softmax(X)\n",
    "    \n",
    "    def get_input_grad(self, Y, T):\n",
    "        \"\"\"Return the gradient at the inputs of this layer.\"\"\"\n",
    "        return (Y - T) / Y.shape[0]\n",
    "    \n",
    "    def get_cost(self, Y, T):\n",
    "        \"\"\"Return the cost at the output of this output layer.\"\"\"\n",
    "        return - np.multiply(T, np.log(Y)).sum() / Y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a sample model to be trained on the data\n",
    "hidden_neurons_1 = 20  # Number of neurons in the first hidden-layer\n",
    "hidden_neurons_2 = 20  # Number of neurons in the second hidden-layer\n",
    "# Create the model\n",
    "layers = [] # Define a list of layers\n",
    "# Add first hidden layer\n",
    "layers.append(LinearLayer(X_train.shape[1], hidden_neurons_1))\n",
    "layers.append(LogisticLayer())\n",
    "# Add second hidden layer\n",
    "layers.append(LinearLayer(hidden_neurons_1, hidden_neurons_2))\n",
    "layers.append(LogisticLayer())\n",
    "# Add output layer\n",
    "layers.append(LinearLayer(hidden_neurons_2, T_train.shape[1]))\n",
    "layers.append(SoftmaxOutputLayer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the forward propagation step as a method.\n",
    "def forward_step(input_samples, layers):\n",
    "    \"\"\"\n",
    "    Compute and return the forward activation of each layer in layers.\n",
    "    Input:\n",
    "        input_samples: A matrix of input samples (each row is an input vector)\n",
    "        layers: A list of Layers\n",
    "    Output:\n",
    "        A list of activations where the activation at each index i+1 corresponds to\n",
    "        the activation of layer i in layers. activations[0] contains the input samples.  \n",
    "    \"\"\"\n",
    "    activations = [input_samples] # List of layer activations\n",
    "    # Compute the forward activations for each layer starting from the first\n",
    "    X = input_samples\n",
    "    for layer in layers:\n",
    "        Y = layer.get_output(X)  # Get the output of the current layer\n",
    "        activations.append(Y)  # Store the output for future processing\n",
    "        X = activations[-1]  # Set the current input as the activations of the previous layer\n",
    "    return activations  # Return the activations of each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the backward propagation step as a method\n",
    "def backward_step(activations, targets, layers):\n",
    "    \"\"\"\n",
    "    Perform the backpropagation step over all the layers and return the parameter gradients.\n",
    "    Input:\n",
    "        activations: A list of forward step activations where the activation at \n",
    "            each index i+1 corresponds to the activation of layer i in layers. \n",
    "            activations[0] contains the input samples. \n",
    "        targets: The output targets of the output layer.\n",
    "        layers: A list of Layers corresponding that generated the outputs in activations.\n",
    "    Output:\n",
    "        A list of parameter gradients where the gradients at each index corresponds to\n",
    "        the parameters gradients of the layer at the same index in layers. \n",
    "    \"\"\"\n",
    "    param_grads = collections.deque()  # List of parameter gradients for each layer\n",
    "    output_grad = None  # The error gradient at the output of the current layer\n",
    "    # Propagate the error backwards through all the layers.\n",
    "    #  Use reversed to iterate backwards over the list of layers.\n",
    "    for layer in reversed(layers):   \n",
    "        Y = activations.pop()  # Get the activations of the last layer on the stack\n",
    "        # Compute the error at the output layer.\n",
    "        # The output layer error is calculated different then hidden layer error.\n",
    "        if output_grad is None:\n",
    "            input_grad = layer.get_input_grad(Y, targets)\n",
    "        else:  # output_grad is not None (layer is not output layer)\n",
    "            input_grad = layer.get_input_grad(Y, output_grad)\n",
    "        # Get the input of this layer (activations of the previous layer)\n",
    "        X = activations[-1]\n",
    "        # Compute the layer parameter gradients used to update the parameters\n",
    "        grads = layer.get_params_grad(X, output_grad)\n",
    "        param_grads.appendleft(grads)\n",
    "        # Compute gradient at output of previous layer (input of current layer):\n",
    "        output_grad = input_grad\n",
    "    return list(param_grads)  # Return the parameter gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gradient errors found\n"
     ]
    }
   ],
   "source": [
    "# Perform gradient checking\n",
    "nb_samples_gradientcheck = 10 # Test the gradients on a subset of the data\n",
    "X_temp = X_train[0:nb_samples_gradientcheck,:]\n",
    "T_temp = T_train[0:nb_samples_gradientcheck,:]\n",
    "# Get the parameter gradients with backpropagation\n",
    "activations = forward_step(X_temp, layers)\n",
    "param_grads = backward_step(activations, T_temp, layers)\n",
    "\n",
    "# Set the small change to compute the numerical gradient\n",
    "eps = 0.0001\n",
    "# Compute the numerical gradients of the parameters in all layers.\n",
    "for idx in range(len(layers)):\n",
    "    layer = layers[idx]\n",
    "    layer_backprop_grads = param_grads[idx]\n",
    "    # Compute the numerical gradient for each parameter in the layer\n",
    "    for p_idx, param in enumerate(layer.get_params_iter()):\n",
    "        grad_backprop = layer_backprop_grads[p_idx]\n",
    "        # + eps\n",
    "        param += eps\n",
    "        plus_cost = layers[-1].get_cost(forward_step(X_temp, layers)[-1], T_temp)\n",
    "        # - eps\n",
    "        param -= 2 * eps\n",
    "        min_cost = layers[-1].get_cost(forward_step(X_temp, layers)[-1], T_temp)\n",
    "        # reset param value\n",
    "        param += eps\n",
    "        # calculate numerical gradient\n",
    "        grad_num = (plus_cost - min_cost)/(2*eps)\n",
    "        # Raise error if the numerical grade is not close to the backprop gradient\n",
    "        if not np.isclose(grad_num, grad_backprop):\n",
    "            raise ValueError('Numerical gradient of {:.6f} is not close to the backpropagation gradient of {:.6f}!'.format(float(grad_num), float(grad_backprop)))\n",
    "print('No gradient errors found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the minibatches\n",
    "batch_size = 25  # Approximately 25 samples per batch\n",
    "nb_of_batches = X_train.shape[0] / batch_size  # Number of batches\n",
    "# Create batches (X,Y) from the training set\n",
    "XT_batches = zip(\n",
    "    np.array_split(X_train, nb_of_batches, axis=0),  # X samples\n",
    "    np.array_split(T_train, nb_of_batches, axis=0))  # Y targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a method to update the parameters\n",
    "def update_params(layers, param_grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Function to update the parameters of the given layers with the given gradients\n",
    "    by gradient descent with the given learning rate.\n",
    "    \"\"\"\n",
    "    for layer, layer_backprop_grads in zip(layers, param_grads):\n",
    "        for param, grad in zip(layer.get_params_iter(), layer_backprop_grads):\n",
    "            # The parameter returned by the iterator point to the memory space of\n",
    "            #  the original layer and can thus be modified inplace.\n",
    "            param -= learning_rate * grad  # Update each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform backpropagation\n",
    "# initalize some lists to store the cost for future analysis        \n",
    "minibatch_costs = []\n",
    "training_costs = []\n",
    "validation_costs = []\n",
    "\n",
    "max_nb_of_iterations = 300  # Train for a maximum of 300 iterations\n",
    "learning_rate = 0.1  # Gradient descent learning rate\n",
    "\n",
    "# Train for the maximum number of iterations\n",
    "for iteration in range(max_nb_of_iterations):\n",
    "    for X, T in XT_batches:  # For each minibatch sub-iteration\n",
    "        activations = forward_step(X, layers)  # Get the activations\n",
    "        minibatch_cost = layers[-1].get_cost(activations[-1], T)  # Get cost\n",
    "        minibatch_costs.append(minibatch_cost)\n",
    "        param_grads = backward_step(activations, T, layers)  # Get the gradients\n",
    "        update_params(layers, param_grads, learning_rate)  # Update the parameters\n",
    "    # Get full training cost for future analysis (plots)\n",
    "    activations = forward_step(X_train, layers)\n",
    "    train_cost = layers[-1].get_cost(activations[-1], T_train)\n",
    "    training_costs.append(train_cost)\n",
    "    # Get full validation cost\n",
    "    activations = forward_step(X_validation, layers)\n",
    "    validation_cost = layers[-1].get_cost(activations[-1], T_validation)\n",
    "    validation_costs.append(validation_cost)\n",
    "    if len(validation_costs) > 3:\n",
    "        # Stop training if the cost on the validation set doesn't decrease\n",
    "        #  for 3 iterations\n",
    "        if validation_costs[-1] >= validation_costs[-2] >= validation_costs[-3]:\n",
    "            break\n",
    "    \n",
    "nb_of_iterations = iteration + 1  # The number of iterations that have been executed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-8d9cc735e660>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0miteration_x_inds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_of_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_of_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Plot the cost over the iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mminibatch_x_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch_costs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'k-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cost minibatches'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration_x_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_costs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cost full training set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration_x_inds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_costs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b-'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cost validation set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/itetsu/anaconda/lib/python3.5/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   3159\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3160\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3161\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3162\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3163\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwashold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/itetsu/anaconda/lib/python3.5/site-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1817\u001b[0m                     warnings.warn(msg % (label_namer, func.__name__),\n\u001b[1;32m   1818\u001b[0m                                   RuntimeWarning, stacklevel=2)\n\u001b[0;32m-> 1819\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1820\u001b[0m         \u001b[0mpre_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1821\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpre_doc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/itetsu/anaconda/lib/python3.5/site-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_alias_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0mlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/itetsu/anaconda/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_grab_next_args\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                     \u001b[0;32myield\u001b[0m \u001b[0mseg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/itetsu/anaconda/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[0;34m(self, tup, kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_xy_from_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommand\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'plot'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/itetsu/anaconda/lib/python3.5/site-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m_xy_from_xy\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y must have same first dimension\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x and y can be no greater than 2-D\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgsAAAFkCAYAAACuFXjcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAOgSCwBA1/8DBAe+bZJxNlwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10cea22b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the minibatch, full training set, and validation costs\n",
    "minibatch_x_inds = np.linspace(0, nb_of_iterations, num=nb_of_iterations*nb_of_batches)\n",
    "iteration_x_inds = np.linspace(1, nb_of_iterations, num=nb_of_iterations)\n",
    "# Plot the cost over the iterations\n",
    "plt.plot(minibatch_x_inds, minibatch_costs, 'k-', linewidth=0.5, label='cost minibatches')\n",
    "plt.plot(iteration_x_inds, training_costs, 'r-', linewidth=2, label='cost full training set')\n",
    "plt.plot(iteration_x_inds, validation_costs, 'b-', linewidth=3, label='cost validation set')\n",
    "# Add labels to the plot\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('$\\\\xi$', fontsize=15)\n",
    "plt.title('Decrease of cost over backprop iteration')\n",
    "plt.legend()\n",
    "x1,x2,y1,y2 = plt.axis()\n",
    "plt.axis((0,nb_of_iterations,0,2.5))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on the test set is 0.37\n"
     ]
    }
   ],
   "source": [
    "# Get results of test data\n",
    "y_true = np.argmax(T_test, axis=1)  # Get the target outputs\n",
    "activations = forward_step(X_test, layers)  # Get activation of test samples\n",
    "y_pred = np.argmax(activations[-1], axis=1)  # Get the predictions made by the network\n",
    "test_accuracy = metrics.accuracy_score(y_true, y_pred)  # Test set accuracy\n",
    "print('The accuracy on the test set is {:.2f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Show confusion table\n",
    "conf_matrix = metrics.confusion_matrix(y_true, y_pred, labels=None)  # Get confustion matrix\n",
    "# Plot the confusion table\n",
    "class_names = ['${:d}$'.format(x) for x in range(0, 10)]  # Digit class names\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "# Show class labels on each axis\n",
    "ax.xaxis.tick_top()\n",
    "major_ticks = range(0,10)\n",
    "minor_ticks = [x + 0.5 for x in range(0, 10)]\n",
    "ax.xaxis.set_ticks(major_ticks, minor=False)\n",
    "ax.yaxis.set_ticks(major_ticks, minor=False)\n",
    "ax.xaxis.set_ticks(minor_ticks, minor=True)\n",
    "ax.yaxis.set_ticks(minor_ticks, minor=True)\n",
    "ax.xaxis.set_ticklabels(class_names, minor=False, fontsize=15)\n",
    "ax.yaxis.set_ticklabels(class_names, minor=False, fontsize=15)\n",
    "# Set plot labels\n",
    "ax.yaxis.set_label_position(\"right\")\n",
    "ax.set_xlabel('Predicted label')\n",
    "ax.set_ylabel('True label')\n",
    "fig.suptitle('Confusion table', y=1.03, fontsize=15)\n",
    "# Show a grid to seperate digits\n",
    "ax.grid(b=True, which=u'minor')\n",
    "# Color each grid cell according to the number classes predicted\n",
    "ax.imshow(conf_matrix, interpolation='nearest', cmap='binary')\n",
    "# Show the number of samples in each cell\n",
    "for x in xrange(conf_matrix.shape[0]):\n",
    "    for y in xrange(conf_matrix.shape[1]):\n",
    "        color = 'w' if x == y else 'k'\n",
    "        ax.text(x, y, conf_matrix[y,x], ha=\"center\", va=\"center\", color=color)       \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
